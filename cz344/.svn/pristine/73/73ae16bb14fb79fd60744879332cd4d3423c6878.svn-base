function [bestC,bestP,bestval,allvalerrs]=crossvalidate(xTr,yTr,ktype,Cs,paras)
% function [bestC,bestP,bestval,allvalerrs]=crossvalidate(xTr,yTr,ktype,Cs,paras)
%
% INPUT:	
% xTr : dxn input vectors
% yTr : 1xn input labels
% ktype : (linear, rbf, polynomial)
% Cs   : interval of regularization constant that should be tried out
% paras: interval of kernel parameters that should be tried out
% 
% Output:
% bestC: best performing constant C
% bestP: best performing kernel parameter
% bestval: best performing validation error
% allvalerrs: a matrix where allvalerrs(i,j) is the validation error with parameters Cs(i) and paras(j)
%
% Trains an SVM classifier for all possible parameter settings in Cs and paras and identifies the best setting on a
% validation split. 
%
[~, nC] = size(Cs);
[~, nP] = size(paras);
allvalerrs = zeros(nC, nP);
%% Split off validation data set
% YOUR CODE
[~,n] = size(yTr);
num = n / 10;
idx=randperm(n);
idx=idx(1:num);
yVa1=yTr(:,idx);
xVa1=xTr(:,idx);
yTr1=yTr;
xTr1=xTr;
yTr1(:,idx)=[];
xTr1(:,idx)=[];

idx=randperm(n);
idx=idx(1:num);
yVa2=yTr(:,idx);
xVa2=xTr(:,idx);
yTr2=yTr;
xTr2=xTr;
yTr2(:,idx)=[];
xTr2(:,idx)=[];



%% Evaluate all parameter settings
% YOUR CODE
bestC = 0;
bestP = 0;
bestval = 1;
for i=1:nC
  for j=1:nP
    svmclassify=trainsvm(xTr1,yTr1,Cs(i),'rbf', paras(j));
    allvalerrs(i,j) += sum(sign(svmclassify(xVa1))~=yVa1(:))./length(yVa1);
    svmclassify=trainsvm(xTr2,yTr2,Cs(i),'rbf', paras(j));
    allvalerrs(i,j) += sum(sign(svmclassify(xVa2))~=yVa2(:))./length(yVa2);
    allvalerrs(i,j) /= 2;
    if(allvalerrs(i,j) < bestval)
      bestval = allvalerrs(i,j);
      bestC = Cs(i);
      bestP = paras(j);
    end;
  end;
end;
    

%% Identify best setting
% YOUR CODE


